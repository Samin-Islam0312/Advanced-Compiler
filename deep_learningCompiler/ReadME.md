
# Lowering and performance optimization" (tiling). 

It should end up generating PTX assembly code for the H100 (sm_90), but I was unable to run the complete thimg and see the results due to resource issues.

```Bash
#!/bin/bash
set -e # Exit immediately if a command fails

# Input file generated by the python script
INPUT="tiny_mlp.mlir"

echo "Step 1: Lowering Torch Dialect -> Linalg Dialect"
# Converts PyTorch operations into generic linear algebra operations
torch-mlir-opt $INPUT \
    --torch-decompose-complex-ops \
    --torch-backend-to-linalg-on-tensors-backend-pipeline \
    -o tiny_mlp_linalg.mlir

echo "Step 2: Optimization (Tiling)"
# Performance Optimization: Tiling the matrix multiplications
# I tried with tile sizes 2,2,2 because the model is tiny (4x8 input)
mlir-opt tiny_mlp_linalg.mlir \
    --linalg-tile="tile-sizes=2,2,2" \
    --linalg-generalize-named-ops \
    -o tiny_mlp_tiled.mlir

echo "Step 3: Lowering to GPU (NVVM)"
# Standard pipeline: Linalg -> Affine -> SCF -> GPU -> NVVM
mlir-opt tiny_mlp_tiled.mlir \
    --one-shot-bufferize="bufferize-function-boundaries" \
    --convert-linalg-to-affine-loops \
    --func-bufferize \
    --gpu-kernel-outlining \
    --lower-affine \
    --convert-scf-to-cf \
    --convert-gpu-to-nvvm \
    --reconcile-unrealized-casts \
    -o tiny_mlp_gpu.mlir

echo "Step 4: Generating PTX Assembly (sm_90 for H100)"
# Translation to LLVM IR and then to PTX assembly
mlir-translate tiny_mlp_gpu.mlir \
    --mlir-to-llvmir \
    | llc -march=nvptx64 -mcpu=sm_90 -mattr=+ptx80 -o tiny_mlp.ptx

echo "few lines of the compiled code:"
head -n 15 tiny_mlp.ptx
```

## How to Run It
Generate the model:
```Bash
python generate_ir.py
```

## Run the compiler pipeline:
```Bash
bash compile.sh
```
